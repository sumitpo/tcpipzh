\chapter{TCP拥塞控制}

\section{引言}
本章将探讨 TCP 实现拥塞控制的方法，这也是批量数据传输中最重要的。拥塞控制是TCP 通信的每一方需要执行的一素列行。这些行为由特定算法规定，用于防止网络因为大
规模的通信负载而瘫痪。其基本方法是当有理由认为网络即将进入拥塞状态（或者已经由于拥塞而出现路由器丢包情况）时减绥 TCP传输。TCP 拥塞控制的难点在于怎样准确地判断何
时需要减缓且如何减缓 TCP 传输，以及何时恢复其原有的速度。

TCP 是提供系统间数据可靠传输服务的协议。第15章已经提到，当 TCP 通信的接收方的接收速度无法匹配发送速度时，发送方会降低发送速度。TCP 的流量控制机制完成了对发
送速率的调节，它是基于 ACK 数据包中的通告窗口大小字段来实现的。这种方式提供了明确的接收方返回的状态信息，避免接收方缓存谧出。

当网络中大量的发送方和接收方被要求承担超负荷的通信任务时，可以考虑采取降低发送速率或者最终丢弃部分数据（也可将两者结合使用）的方法。这是将排队理论应用于路由
器的基本观测结果：即使路由器能够存储一些数据，但若源源不断的数据到达速率高于发出速率，任何容量的中间存储都会溢出。简言之，当某一路由器在单位时间内接收到的数据量
多于其可发送的数据量时，它就需要把多余的部分存储起来。假如这种状况持续，最终存储资源将会耗尽，路由器因此只能丢弃部分数据。

路由器因无法处理高速率到达的流量而被迫丢弃数据信息的现象称为拥塞。当路由器处于上述状态时，我们就说出现了拥塞。即使仅有一条通信连接，也可能造成一个甚至多个路
由器拥塞。若不采取对策，网络性能将大受影响以致瘫痪。在最坏情况下，甚至形成拥塞崩溃。为避免或者在一定程度上缓解这种状况，TCP 通信的每一方实行拥塞控制机制。不同的
TCP 版本（包括运行 TCP/IP 协议栈的操作系统）采取的规程和行为有所差异。本章将着重讨论最常用的方法。
\subsection{TCP 拥塞检测}
如前所述，针对丢包情况，TCP 采取的首要机制是重传，包括超时重传和快速重传（参见第14 章）。考虑如下情形，当网络处于拥塞崩溃状态时，共用一条网络传输路径的多个
TCP 连接却需要重传更多的数据包。这就好比火上浇油，可想而知，结果只会更糟，所以这种情况应该尽量避免。

当拥塞状况出现（或将要出现）时，我们可以减缓 TCP 发送端的发送速率；若拥塞情况有所缓解，可以检测和使用新的可用带宽。然而这在互联网中却很难做到，因为对于TCP
发送方来说，没有一个精确的方法去知晓中间路由器的状态。换言之，没有一个明确的信号告知拥塞状况已发生。典型的TCP 只有在断定拥塞发生的情况下，才会采取相应的行动。
推断是否出现拥塞，通常看是否有丢包情况发生。在TCP 中，丢包也被用作判断拥塞发生与否的指标，用来衡量是否实施相应的响应措施（即以某种方式减级发送）。从20世纪80年
代起，TCP 就一直沿用这种方法。其他拥塞探测方法，包括时延测量和显式拥塞通知（ECN，16.11 节会讨论），使得 TCP能在丢包发生前检测拥塞。在学习一些“经典”算法后，我们将
讨论上述探测方法。

\begin{tcolorbox}
    在当今的有线网络中，出现在路由器或交换机中的拥塞是造成丟包的主要原因。而在无线网络中，传输和接收错误是导致丟包的重要因素。从20世纪90年代
    中期无线网络荻得广泛应用开始，判断丢包是由于拥塞引起还是传输错误引起，一直是研究的热点问题。
\end{tcolorbox}

在第14章中，我们已经看到 TCP 如何利用计时器、确认以及选择确认机制来检测丢包和恢复传输。当有丢包情况出现时，TCP的任务是重传这些数据包。现在我们关心的是，当
观测到丢包后，TCP还做了哪些工作，特别是它如何识别这就是已出现拥塞的信号，进而需要执行减速操作。下面的章节主要讨论 TCP 何时减速以及怎样减速（包括如何恢复传输速
度）。我们首先介绍 TCP 在建立新连接时如何确立基本数据传输速率，以及稳定执行大数据量传输操作的经典算法。另外，我们也整合了近年来对这些算法的研究和改进成果，并细查
了相关扩展资料。在此基础上，我们讨论总结了 TCP 拥塞控制安全和其他相关问题。拥塞控制是网络研究领域的热点［RFC6077］，每年都会有相关论文发表。

\subsection{减缓TCP发送}
一个亟待解决的问题是，如何减缓TCP发送。在第15章已经提到，根据接收方剩余缓存空间大小，在TCP 头部设置了通知窗口大小字段，该数值是 TCP 发送方调节发送速率的
依据。进一步说，当接收速率或网络传输速率过慢时，我们需要降低发送速率。实现上述操作，基于对网络传输能力的估计，可以在发送端引入一个窗口控制变量，确保发送窗口大
小不超过接收端接收能力和网络传输能力，即TCP发送端的发送速率等于接收速率和传输速率两者中较小值。

反映网络传输能力的变量称为拥塞窗口（congestion window），记作cwnd。因此，发送端实际（可用）窗口 W就是接收端通知窗口 awnd 和拥塞窗口cwnd 的较小者：

\begin{equation}
    W = min (cwnd, awnd)
\end{equation}

根据上述等式，TCP发送端发送的数据中，还没有收到ACK 回复的数据量不能多于P（以包或字节为单位）。这种已经发出但还未经确认的数据量大小有时称为在外数据值（flight
size），它总是小于等于 W。通常，W可以以包或字节为单位。

\begin{tcolorbox}
    当TCP 不使用选择确认机制时，W的限制作用体现为，发送方发送的报文段序列号不能大于ACK号的最大值与 W之和。而对采用选择确认的发送方则有所
    不同，W被用来限制在外数据值。
\end{tcolorbox}

这看似合乎逻辑，但实际并非如此。因为网络和接收端状况会随时间变化，相应地，awnd 和cwnd 的数值也会随之改变。另外，由于缺少显示拥塞的明确信号（参见前述章节），
TCP发送方无法直接获得cwnd 的“准确”值。因此，变量W、cwnd、awnd 的值都要根据经验设定并需动态调节。此外，如前所述，P的值不能过大或过小—一我们希望其接近带宽
延迟积（Bandwidth-Delay Product, BDP），也称作最佳窗口大小（optimal window size）。W反映网络中可存储的待发送数据量大小，其计算值等于 RTT 与链路中最小通行速率（即发
送端与接收端传输路径中的“瓶颈”）的乘积。逼常的策略是，为使网络资源得到高效利用，应保证在网络申传输的数据量达到BDP。但若在传偷数据值远高于BDP时，会引人不必要
的延时（参见16.10节），所以这也是不可取的。在网络中如何确定一个连接的BDP是难点，需要考惠诸多因素，如路由、时延、统计复用（即共用传倫资派）水平随时间的变化性等。

\begin{tcolorbox}
    这里我们主要讨论由TCP 发送方的数据发送而产生的拥塞，但也要注意因接收方回复ACK而产生的相反方向链路上的删塞，目前也有相关研究针对孩一问
    题。在文献TRFC5690］中介绍了一种方法，该方法中TCP接收方需要很据一定比率国复ACK（即接收了多少个数据包后才能发送一个ACK）。
\end{tcolorbox}

\section{一些经典算法}
当一个新的TCP连接建立之初，还无法狄知可用的传输资源，所以cwnd 的初始價也无法确定。（也有一些例外，如有些系统的缓存容量是预先设定的，在第14 章我们称其为目
的度量（destination metrie）。）TCP 通过与接收端交换一个数据包就能获得awnd 的值，不需要任何明确的信号。显而易见，狄得cwnd 最佳值的唯一方法是以越来越快的速率不断发送
数据，直到出现数据包丢失（或网络圳塞）为止。这时考虑立即以可用的最大速率发送（受awnd 的限制），或是慢速启动发送。由于多个 TCP 连接共享一个网络传输路径，以全速启动
会影响其他连接的传输性能，所以通常会有特定的算法来避免过快启动，直至稳定传输后才会运行相应的其他算法。

TCP 发送方的拥塞控制操作是由ACK 的接收来驱动或“控制”的。当 TCP 传输处于稳定阶段（cwnd 取合适值），接收到 ACK 回复表明发送的数据包已被成功接收，因此可以继续
发送操作。据此推理，稳定状态下的 TCP抑塞行为，实际是试图使在网络传输路径上的数据包守恒（参见图 16-1）。这里的守恒是从物理学意义上而言的——某个量（如动量、能量）
进入一个系统不会凭室消失或出现，而是以某种表现形式继续存在。

TCP拥塞控制操作是基于数据包守恒原理运行的由子传输能力有限，数据包（P）会适时地“伸展”。接收方以一定问隔（P）接收到数据包后，会陆续（以4，为间隔）生成相应的
ACK，以一定的发送间隔（小）返回给发送方。当ACK 陆线（以A.为间隔）到达发送端时，其到达提供了一个信号或者说“ACK 时钟”，表明发送嘴可以继续发送数帮。在稳定传输状
态下，整个系统可“自同步"控制（本图改编自 ［I88］，源丁S. Seshan' s CMU Leeture Notes，2005.3.22）

如图16-1 所示，上下两条通道形似“漏汁”。发送方发送的（校大）数擔包经上通道传输给接收方。相对较狄窄部分表不传输较慢的连接链路，数娜包需要适时地被“伸肥”。两
端部分（位于发送方和接收方）让数据包发送前和接收后的队列。下通道传输相应ACK数据包。在高效传输的稳定状态下，上下通道都不会出现包堵塞的情况，而且在上通道中也不会
有较大传输间隔。注意到发送方接收到一个 ACK 就表明可向图16-1 中的上层通道发送一个数据包（即网络中可容纳另一个包）。这种由一个 ACK 到达（称作 ACK 时钟）触发一个新数
据包传输的关系称为自同步（sclf-clocking）。

现在我们讨论 TCP 的两个核心算法：慢启动和拥塞避免。这两个算法是基于包守恒和ACK 时钟原理，最早在 Jacobson ［J88］的经典论文里被正式提出。几年后，Jacobson 对拥塞
避免算法提出了改进［J90］。这两个算法不是同时运行的—一在任一给定时刻，TCP 只运行一个算法，但两者可以相互切换。下面我们将详细讨论这两个算法，包括如何使用以及对算
法的改进。每个 TCP连接都能独立运行这两个算法。

\subsection{慢启动}
当一个新的TCP 连接建立或检测到由重传超时（RTO）导致的丢包时，需要执行慢启动。TCP 发送端长时间处于空闲状态也可能调用慢启动算法。慢启动的目的是，使TCP 在
用拥塞避免探寻更多可用带宽之前得到cwnd值，以及帮助TCP建立ACK 时钟。通常，TCP 在建立新连接时执行慢启动，直至有丢包时，执行拥塞避免算法（参见16.2.2节）进入
稳定状态。下文引自 ［RFC5681］：

在传输初始阶段，由于未知网络传输能力，需要缓慢探测可用传输资源，防止短时间内大量数据注入导致拥塞。慢启动算法正是针对这一问题而设计。在数据传
输之初或者重传计时器检测到丢包后，需要执行慢启动。

TCP 以发送一定数目的数据段开始慢启动（在SYN交换之后），称为初始窗口（InitialWindow, IW）。IW 的值初始设 一个 SMSS（发送方的最大段大小），但在［RFCS681］中设
为一个稍大的值，计算公式如下：

\begin{equation}
    IW =2* （SMSS）且小于等于2个数据段（当SMSS>2190字节）
    IW =3* （SMSS）且小于等于3个数据段（当2190≥SMSS>1095字节）
    IW =4*（SMSS）且小于等于4个数据段（其他）
\end{equation}

上述 IW的计算方式可能使得初始窗口为几个数据包大小（如3个或4个），为简单起见，我们只讨论IW =1 SMSS 的情况。TCP 连接初始的cwnd =1 SMSS，意味着初始可用
窗口W也为1 SMSS。注意到大部分情况下，SMSS 为接收方的 MSS（最大段大小）和路径MTU（最大传输单元）两者中较小值。

假设没有出现丢包情况且每个数据包都有相应的ACK，第一个数据段的ACK 到达，说明可发送一个新的数据段。每接收到一个好的ACK 响应，慢启动算法会以 min （N, SMSS）
来增加 cwnd值。这里的N 是指在未经确认的传输数据中能通过这一“好的ACK”确认的字节数。所谓的“好的ACK”是指新接收的 ACK 号大于之前收到的ACK。

\begin{tcolorbox}
    已被 ACK 确认的字节数目用于支持适当字节计数（Appropriate Byte Counting，ABC）［RFC3465］，这是［RFC5681］推荐的实验规范。ABC 用于计数“ACK 分裂”
    攻击（将在16.12 节叙述），指利用许多较小ACK使TCP 发送方加速发送。Linux 利用布尔系统配置变量 net.jpv4.tep_abe 设定 ABC是否可用（默认不可用）。在最近的
    几个 Windows 版本中，ABC默认开启。
\end{tcolorbox}

因此，在接收到一个数据段的ACK后，通常 cwnd 值会增加到2，接着会发送两个数据段。如果成功收到相应的新的 ACK,cwnd 会由2变4，由4变8，以此类推。一般情况下，
假设没有丢包且每个数据包都有相应 ACK，在k轮后W的值为W=2’，即k=10g2W，需要K个 RTT 时间操作窗口才能达到 W大小。这种增长看似很快（以指数函数增长），但若与一
开始就允许以最大可用速率（即接收方通知窗口大小）发送相比，仍显缓慢。（W不会超过awnd。）

如果假设某个 TCP 连接中接收方的通知窗口非常大（比如说，无穷大），这时 cwnd 就是影响发送速率的主要因素（设发送方有较大发送需求）。如前所述，cwnd 会随着 RTT 呈指数
增长。因此，最终 cwnd（W也如此）会增至很大，大量数据包的发送将导致网络瘫痪（TCP吞吐量与 W/RTT 成正比）。当发生上述情况时，cwnd 将大幅度减小（减至原值一半）。这是
TCP 由慢启动阶段至拥塞避免阶段的转折点，与cwnd 和慢启动圖值（slow start threshold，ssthresh）相关。

图16-2（左）描述了慢启动操作。数值部分以 RTT 为单位。假设该连接首先发送一个包（图上部），返回一个 ACK，接着在第二个 RTT 时间里发送两个包，会接收到两个ACK。
TCP 发送方每接收一个ACK 就会执行一次cwnd 的增长操作，以此类推。右图描述了 cwnd随时间增长的指数函数。图中另一条曲线显示了每两个数据包收到一个ACK 时 cwnd 的增长
情况。通常在 ACK 延时情况下会采用这种方式，这时的 cwnd 仍以指数增长，只是增幅不是很大。正因 ACK 可能会延时到达，所以一些 TCP 操作只在慢启动阶段完成后才返回ACK。
Linux 系统中，这被称为快速确认（“快速 ACK 模式”），从内核版本2.4.4开始，快速确认一直是基本 TCP/P 协议栈的一部分。


经典慢启动算法操作。在没有 ACK 延时情况下，每接收到一个好的ACK 就意味着发送方可以发送两个新的数据包（左）。这会使得发送方窗口随时间垦指数增长（右，上方曲线）。当发生
ACK 延时，如每脂一个數据包生成一个ACK， cwnd 仍3以、指数增长，但增幅较小（右，下方曲线）

\subsubsection{拥塞避免}
如上所述，在连接建立之初以及由超时判定丢包发生的情况下，需要执行慢肩动操作。在慢启动阶段，cwnd 会快速增长，帮助确立一个慢启动阈值。一旦达到阈值，就意味着可
能有更多可用的传输资源。如果立即全部占用这些资源，将会使共享路由器队列的其他连接出现严重的丢包和重传情况，从而导致整个网络性能不稳定。

为了得到更多的传输资源而不致影响其他连接传输，TCP 实现了拥塞避免算法。一旦确立慢启动阈值，TCP 会进入拥塞避免阶段，cwnd 每次的增长值近似于成功传输的数据段大小。
这种随时间线性增长方式与慢启动的指数增长相比级慢许多。更准确地说，每接收一个新的ACK, cwnd 会做以下更新：
\begin{equation}
    cwnd, + 1 = cwnd, + SMSS * SMSS/cwnd,
\end{equation}

分析上式，假设 cwndo =k*SMSS 字节分k段发送，在接收到第一个ACK后，cwnd 的值增长了 1/k倍：
\begin{equation}
    cwnd, = cwndo + SMSS*SMSS/cwndo = k*SMSS + SMSS * (SMSS/ (k * SMSS) )
= k * SMSS + (1/k) *SMSS = (k + (1/k)) *SMSS = cwndo + (1/k) *SMSS
\end{equation}

随着每个新的ACK 到达，cwnd 会有相应的小幅增长（取决于上式中的k值），整体增长率呈现轻微的次线性。尽管如此，我们通常认为拥塞避免阶段的窗口随时间线性增长（见
图 16-3），而慢启动阶段呈指数增长（见图16-2）。这个函数也称为累加增长，因为每成功接收到相应数据，cwnd 就会增加一个特定值（这里大约是一个包大小）。

拥塞避免算法操作。若没有 ACK延时发生，每接收一个好的ACK，就意味着发送方可继续发送1/W个新的数据包。发送窗口随时间近似呈线性增长（右，上方曲线）。当有ACK 延时，
如每隔一个数据包生成一个 ACK，cwnd 仍近似星线性增长，只是增幅较小（右，下方曲线）

图16-3（左）描述了拥塞避免操作。数值部分仍是以 RTT为单位。假设连接发送了4个数据包（图上方），返回了4个ACK，cwnd 可以有相应的增长。在第2个RTT阶段，增长
可达到整数值，使得cwnd增加一个 SMSS，这样可以继续发送一个新的数据包。右图描绘了cwnd 随时间近似呈线性增长。另一曲线模拟 ACK 延时，显示了每两个数据包收到一个
ACK 时cwnd 的增长情况。这时的cwnd仍近似呈线性增长，只是增幅不是很大。

拥塞避免算法假设由比特错误导致包丢失的概率很小（远小于1\%），因此有丢包发生就表明从源端到目的端必有某处出现了拥塞。如果假设不成立，比如在无线网络中，那么即使
没有拥塞 TCP传输也会变慢。另外，cwnd的增大可能会经历多个 RTT，这就需要有充裕的网络资源，并得到高效利用。这些问题还有很大的研究空间，以后我们将会讨论其中一些方法。

\subsection{慢启动和拥塞避免的选择}
在通常操作中，某个 TCP连接总是选择运行慢启动和拥塞避免中的一个，不会出现两者同时进行的情况。现在我们考虑，在任一给定时刻如何决定选用哪种算法。我们已经知
道，慢启动是在连接建立之初以及超时发生时执行的。那么决定使用慢启动还是拥塞避免的关键因素是什么呢？

前面我们已经提到过慢启动阈值。这个值和 cwnd的关系是决定采用慢启动还是拥塞避免的界线。当 cwnd < ssthresh，使用慢启动算法：当 cwnd > ssthresh，需要执行拥塞避免：
而当两者相等时，任何一种算法都可以使用。由上面描述可以得出，慢启动和拥塞避免之间最大的区别在于，当新的ACK 到达时，cwnd怎样增长。有趣的是，慢启动阈值不是固定的，
而是随时间改变的。它的主要目的是，在没有丢包发生的情况下，记住上一次“最好的”操作窗口估计值。换言之，它记录 TCP 最优窗口估计值的下界。

慢启动阈值的初始值可任意设定（如 awnd 或更大），这会使得 TCP 总是以慢启动状态开始传输。当有重传情况发生，无论是超时重传还是快速重传，ssthresh 会按下式改变：
\begin{equation}
    ssthresh = max（在外数据值 /2, 2*SMSS）
\end{equation}
我们已经知道，如果出现重传情况，TCP 会认为操作窗口超出了网络传输能力范围。这时会将慢启动阈值（ssthresh） 减小至当前窗口大小的一半（但不小于 2*SMSS），从而减小最
优窗口估计值。这样通常会导致 ssthresh 减小，但也有可能会使之增大。分析 TCP 拥塞避免的操作流程，如果整个窗口的数据都成功传输，那么cwnd值可以近似增大1 SMSS。因
此，若cwnd 在一段时间范围内已经增大，将 ssthresh 设为整个窗口大小的一半可能使其增大。这种情况发生在当TCP 探测到更多可用带宽时。在慢启动和拥塞避免结合的情况下，
ssthresh 和cwnd 的相互作用使得TCP 拥塞处理行为显现其独有特性。下面我们探讨将两者结合的完整的算法。

\subsection{Tahoe、Reno 以及快速恢复算法}
至此讨论的慢启动和拥塞避免算法，组成了 TCP 拥塞控制算法的第一部分。它们于20世纪80年代末期在加州大学伯克利分校的4.2版本的UNIX 系统中被提出，称为伯克利软件
版本，或BSD UNIX。至此开始了以美国城市名命名各个 TCP版本的习惯，尤其是那些赌博合法的城市。

4.2版本的BSD（称为 Tahoe）包含了一个TCP版本，它在连接之初处于慢启动阶段，若检测到丢包，不论由于超时还是快速重传，都会重新进入慢启动状态。有丢包情况发生
时，Tahoe简单地将 cwnd 减 初始值（当时设为1 SMSS）以达到慢启动目的，直至ewnd增长为 ssthresh。

这种方法带来的一个问题是，对于有较大BDP的链路来说，会使得带宽利用率低下。因为TCP 发送方经重新慢启动，回归到的还是未丢包状态（cwnd 启动初始值设置过小）。
为解决这一问题，针对不同的丢包情况，重新考虑是否需要重回慢启动状态。若是由重复ACK 引起的丢包（引发快速重传），cwnd 值将被设为上一个 ssthresh，而非先前的1 SMSS。
（在大多数 TCP 版本中，超时仍是引发慢启动的主要原因。）这种方法使得TCP 无须重新慢启动，而只要把传输速率减半即可。

进一步讨论较大BDP链路的情况，结合之前提到的包守恒原理，我们可以得出结论，只要接收到 ACK 回复（包括重传 ACK），就有可能传输新的数据包。BSD UNIX 的4.3 BSD
Reno版中的快速恢复机制就是基于上述结论。在恢复阶段，每收到一个 ACK,cwnd 就能（临时）增长1 SMSS，相应地就意味着能发送一个新的数据包。因此拥塞窗口在一段时间内
会急速增长，直到接收一个好的ACK。不重复的（“好的”）ACK 表明 TCP 结束恢复阶段，拥塞已减少到之前状态。TCP Reno 算法得到了广泛应用，并成为“标准 TCP”的基础。

\subsection{标准 TCP}
尽管究竟哪些构成了“标准”TCP还存在争议，但我们讨论过的上述算法毋庸置疑都属于标准 TCP。慢启动和拥塞避免算法通常结合使用，［RFC5681］给出了其基本方法。这个规
范并不要求严格使用这些精确算法，TCP 实现过程仅利用其核心思想。

总结［RFC5681］中的结合算法，在TCP 连接建立之初首先是慢启动阶段（cwnd =IW），ssthresh 通常取一较大值（至少为 awnd）。当接收到一个好的ACK（表明新的数据传输成功），
cwnd 会相应更新：

\begin{equation}
    cwnd += SMSS（若 cwnd < ssthresh）慢启动
    cwnd += SMSS*SMSS/cwnd（若 cwnd>ssthresh）拥塞避免
\end{equation}

当收到三次重复 ACK（或其他表明需要快速重传的信号）时，会执行以下行为：
\begin{enumerate}
    \item ssthresh 更新为大于等式（16-1）中的值。
    \item 启用快速重传算法，将 cwnd 设为（ssthresh +3*SMSS）。
    \item 每接收一个重复 ACK,cwnd 值暂时增加1 SMSS。
    \item 当接收到一个好的ACK，将 cwnd 重设力 ssthresh。
\end{enumerate}

以上第2步和第3步构成了快速恢复。步骤2设置cwnd 大小，首先cwnd 通常会被减为之前值的一半。然后，考虑到每接收一个重复 ACK，就意味着相应的数据包已成功传输
（因此新的数据包就有发送机会），cwnd 值会相应地暂时增大。这一步也可能出现cwnd 加速递减的情况，因为通常 cwnd 会乘以某个值（这里取0.5）来形成新的cwnd。步骤3维持
cwnd 的增大过程，使得发送方可以继续发送新的数据包（在不超过 awnd 的情况下）。步骤4假设 TCP 已完成恢复阶段，所以 ewnd 的临时膨胀也消除了（有时称这一步为“收缩”）。

以下两种情况总会执行慢启动：新连接的建立以及出现重传超时。当发送方长时间处于空闲状态，或者有理由怀疑 cwnd 不能精确反映网络当前拥塞状态（参见16.3.5节）时，也可
能引发慢启动。在这种情况下，cwnd 的初始值将被设为重启窗口（RW）。在文献［RFC5681］中，推荐 RW值为RW=min （IW,cwnd）。其他情况下，慢启动中ownd 初始设为IW。

\section{对标准算法的改进}
经典的标准 TCP 算法在传输控制领域做出了重大贡献，尤其针对网络拥塞崩溃这一难题，取得了显著效果。

\begin{tcolorbox}
    在1986~1988年，网络胡塞崩溃是引起广泛关注的难点问题。1986年10月，作为早期互联网的重要组成部分，NSFNET 主干网出现了一次严亚故障，运行速度
    仅为其应有速度的千分之十（称力 “NSFNET 危机”）。问题的主要形成原因在于对大量的重传没有任何控制操作。持续的拥塞状态导致了亚严重的丢包现象（由于更
    多的重传操作）和吞吐量低下。采用经典拥塞控制算法有效地解决了这一问题。
\end{tcolorbox}


然而，仍然可以找到值得改进的地方。考虑到 TCP 的普遍使用性，越来越多的研究致力于使TCP 在更广泛的环境里更好地工作。下面我们提出几种方法，现在许多 TCP版本也
已经实现。

\subsection{NewReno}
快速恢复带来的一个问题是，当一个传输窗口出现多个数据包丢失时，一旦其中一个包重传成功，发送方就会接收到一个好的 ACK，这样快速恢复阶段中cwnd 窗口的暂时膨胀就
会停止，而事实上丢失的其他数据包可能并未完成重传。导致出现这种状况的ACK 称为局部ACK（partial ACK）。Reno算法在接收到局部 ACK 后就停止拥塞窗口膨胀阶段，并将其
减小至特定值，这种做法可能导致在重传计时器超时之前，传输通道一直处于空闲状态。力理解出现这种情况的原因，我们首先明确，TCP（无选择确认机制）需要通过三个（或重复
阈值）重复 ACK 包作为信号才能触发快速重传机制。假如网络中没有足够的数据包在传输，那么就不可能因丢包而触发快速重传，最终导致重传计时器超时，引发慢启动操作，从而严
重影响网络吞吐性能。

为解决上述问题，［RFC3782］提出了一种改进算法，称为NewReno。该算法对快速恢复做出了改进，它记录了上一个数据传输窗口的最高序列号（即我们在第14章提到的恢复点）。
仅当接收到序列号不小于恢复点的ACK，才停止快速恢复阶段。这样 TCP 发送方每接收一个ACK 后就能继续发送一个新数据段，从而减少重传超时的发生，特别针对一个窗口出现
多个包丢失的情况时。NewReno是现在比较常用的一个 TCP版本，它不会出现经典快速重传的问题，实现起来也没有选择确认（SACK）复杂。然而，当出现上述多个丢包情况时，
利用SACK 机制能比 NewReno获得更好的性能，但需要较为复杂的拥塞控制操作，下面我们会讨论这一问题。

\subsection{采用选择确认机制的TCP拥塞控制}
在TCP 引人SACK 与选择性重复之后，发送方能够更好地确定发送哪个数据段来填补接收方的空缺（参见第14章）。为了填补接收数据的空峡，发送方通常只发送丢失的数据段，
直至完成所有重传。这和前面提到的基本的快速重传/恢复机制有所差别。

在快速重传/恢复情况下，当出现丢包，TCP 发送方只重传它认为已经丢失的包。如果窗口W允许，还可以发送新的数据包。在快速恢复阶段，由于窗口大小会随着每个 ACK的
到达而膨胀，在完成重传后，通常发送方能有更大的窗口发送更多新数据。采用SACK 机制后，发送方可以知晓多个数据段的丢失情况。因为这些数据都在有效窗口内，理论上可
以即时重传。然而，这样可能会在较短时间内向网络中注人大量数据，削弱拥塞控制效果。SACK TCP 会引发以下问题：在恢复阶段，只使用cwnd 作为发送方滑动窗口的界限来表示
发送多少个（以及哪些）数据包是不够的，且选择发送哪些数据包与发送时间紧密相关。换言之，SACK TCP 强调拥塞管理和选择重传机制的分离。传统（无SACK）TCP则将两者结合。

一种实现分离的方法是，除了维护窗口，TCP 还负责记录注入网络的数据量。［RFC3517］称其为管道（pipe）变量，这是对在外数据的估计值。管道变量以字节（或包，依不同实现方
式而定）为单位，记录传输和重传情况（不考虑丢包，将两者同等对待）。假设 awnd 值较大，只要不等式cwnd - pipe ≥ SMSS 成立，在任何时候SACK TCP 均可发送数据。这里cwnd
仍被用来限定可传输至网络中的数据量，但除了窗口本身，网络中数据量的估计值也被记录了。［FF96］ 详细分析比较了SACK TCP 和传统 TCP的拥塞控制方法，并做了相关仿真工作。

\subsection{转发确认（FACK）和速率减半}
对基于 Reno（包括 NewReno）的TCP版本来说，当快速重传结束后cwnd 值减小，在TCP发送新数据之前至少可以接收一半已发送数据返回的ACK。这和检测到丢包后立即将
拥塞窗口值减半相一致。这样 TCP发送端在前一半的 RTT时间内处于等待状态，在后一半RTT 才能发送新数据，这是我们不愿看到的。

在丢包后，为避免出现等待空闲而又不违背将拥塞窗口减半的做法，［MM96］ 提出了转发确认 （forward acknowledgment,FACK）策略。FACK 包含了两部分算法，称为“过度衰减”
（overdamping）和“缓慢衰减”（rampdown）。从最初想法的提出到改进，最终在Hoe 的工作基础上［H96］形成了统一的算法，称为速率减半（rate halving）。为控制算法尽可能有效地运
行，进一步添加了界定参数，完整的算法被称为带界定参数的速率减半（Rate-Halving withBounding Parameters, RHBP）算法 ［PSCRH］。

RHBP 的基本操作是，在一个 RTT 时间内，每接收两个重复ACK,TCP 发送方可发送一个新数据包。这样在恢复阶段结束前，TCP 已经发送了一部分新数据，与之前的所有发送
都挤在后半个 RTT 时间段内相比，数据发送比较均衡。由于过度集中的发送操作可能持续多个 RTT，对路由缓存造成负担，因此均衡发送是比较有利的。

为了记录较为精确的在外数据估计值，RHBP 利用SACK信息决定FACK 策略：已知的最大序列号的数据到达接收方时，在外数据值加1。注意区分即将发送数据的最大序列号
（图15-9中的 SND.NXT），FACK 给出的在外数据估计值不包括重传。

RHBP 中区分了调整间隔（adjustment interval,cwnd 的修正阶段）和恢复间隔（repairinterval，数据重传阶段）。一旦出现丢包或其他拥塞信号就立即进人调整间隔。调整间隔结
束后 cwnd 的最终值为：至检测时间为止，网络中已正确传输的窗口数据量的一半。RHBP要求发送方传输数据需满足下式；
\begin{equation}
    (SND.NXT - fack + retran_ data + len) < cwnd
\end{equation}

上面的等式得到了包括重传的在外数据值，确保当继续发送一个len 长度的新数据，也不会超过cwnd。假设在 FACK之前的数据已经不在网络中（如丢失或被接收），这样cwnd
就能很好地控制SACK发送方的发送。然而由于SACK 的选择确认特性，可能导致数据包的传输次序过度重排。

Linux 系统实现了 FACK 和速率减半，并默认启用。若 SACK 开启，并将布尔配置变量net.jpv4.tcp_fack 置1，就会激活 FACK。当检测到网络中出现数据包失序，FACK的进一步
行为将被禁用。

速率减半是调节发送操作或避免集中发送的方法之一。我们已经了解了它的优点，但这种方法仍然存在一些问题。［ASA00］利用仿真的方法，详细分析了 TCP 发送调度，结果显示
在很多情况下，它的性能劣于 TCP Reno。另外，研究表明，在接收窗口限制 TCP 连接的情况下，速率减半方法收效甚微 ［MM05］。

\subsection{限制传输}
［RFC3042］ 提出了限制传输（Iimited transmit），它对TCP 做出了微小改进，目的在于使TCP能在可用窗口较小的情况下更好工作。之前已经提到，在Reno 算法中，通常需要三次
重复 ACK 表明数据包丢失。在窗口较小的情况下，当出现丢包，网络中可能没有足够的包去引发快速重传/恢复机制。

采用限制传输策略，TCP发送方每接收两个连续的重复 ACK，就能发送一个新数据包。这就使得网络中的数据包维持一定数量——足以触发快速重传。TCP 因此也可以避免长时
间等待 RTO（可能达到几百毫秒，相对时间较长）而导致吞吐性能下降。限制传输已经成为TCP推荐策略。速率减半也是限制传输的一种形式。

\subsection{拥塞窗口校验}
TCP拥塞管理可能会出现一个问题，那就是发送端可能在一段时间内停止发送（由于没有新数据需要发送或者其他原因阻住发送）。通常情况下，发送操作不会暂停。发送端发送
数据，同时接收 ACK 反馈，以此估计一定时间内（一个 RTT）的 cwnd 和 ssthresh。

在发送操作持续一段时间后，cwnd 可能会增至一个较大值。若发送需要暂停（一定时间后会恢复），根据此时cwnd 的值，在暂停前发送方仍可向网络中（高速）注人大量数据。
若暂停时间足够长，之前的cwnd 可能无法准确反映路径中的拥塞状况。

［RFC2861］ 提出了一种拥塞窗口校验 （Congestion Window Validation，cwv）机制。在发送长时间暂停的情况下，由ssthresh 维护 cwnd 保存的“记忆”，之后cwnd 值会衰减。为
理解这种机制，需要区分空闲（idle）发送端和应用受限（application-limited）发送端。对空闲发送端而言，没有发送新数据的需求，之前发送的数据也已经成功接收ACK。因此，整
个连接处于空闲状态——除了必要的窗口更新外（参见第15章），没有数据和ACK 的传输。应用受限发送端则需要传输数据，但由于某种原因无法发送（可能由于处理器正忙或者下层
阻住数据发送）。这种情况会导致连接利用率低下，但并非完全空闲，之前已发送数据返回的ACK 仍可传输。

CwV算法原理如下：当需要发送新数据时，首先看距离上次发送操作是否超过一个RTO。如果超过，则

\begin{itemize}
    \item 更新 ssthresh 值—设 max （ssthresh， （3/4） *cwnd）。
    \item 每经一个空闲 RTT 时间，cwnd 值就减半，但不小于1 SMSS。
\end{itemize}
对于应用受限阶段（非空闲阶段），执行相似的操作：
\begin{itemize}
    \item 已使用的窗口大小记为 W_used。
    \item 更新 ssthresh 值一—设为 max （ssthresh， （3/4） *cwnd）。
    \item cwnd设力cwnd 和W_used 的平均值。
\end{itemize}

上述操作均减小了cwnd，但 ssthresh 维护了cwnd 的先前值。第一种情况中，如果传输通道长时间空闲，cwnd 将会显著减小。在某些情况下，这种拥塞窗口的处理方法可以取
得更好效果。根据作者的研究，避免空闲阶段可能发生的大数据量注入，可以减轻对有限的路由缓存的压力，从而减少丢包情况的产生。注意到CWV 减小了cwnd值，但没有减小
ssthresh，因此采用这种算法的通常结果是，在长时间发送暂停后，发送方会进入慢启动阶段。Linux TCP 实现了CWV 并默认启用。

\section{伪 RTO 处理——Eifel 响应算法}
在第15章已经提到，若TCP 出现突发的延时，即使没有出现丢包，也可能造成重传超时的假象。这种伪重传现象的发生可能由于链路层的某些变化（如蜂宽转换），也可能是由
于突然出现严重拥塞造成 RTT大幅增长。当出现重传超时，TCP 会调整 ssthresh 并将 cwnd置为IW，从而进人慢启动状态。假如没有出现实际丢包，在RTO 之后到达的ACK 会使得
cwnd快速增大，但在 cwnd 和ssthresh 值重新稳定前，仍然会有不必要的重传，浪费传输资源。

针对上述问题已有相关探测方法。我们在第14章讨论了其中的一些方法（如DSACK、Eifel、F-RTO）。其中任一探测方法只要结合相关响应算法，就能“还原”TCP 对拥塞控
制变量的操作。一种比较常用（即在IETF 标准化过程中）的响应算法就是Eifel 响应算法［RFC4015］。

Eifel 算法包含检测算法和响应算法两部分，两者在理论上是独立的。任何使用 Eifel 响应算法实现的TCP操作，必须使用相应的标准操作规范或实验 RFC（即被记录的RFC）中
规定的检测算法。

Eifel 响应算法用于处理重传计时器以及重传计时器超时后的拥塞控制操作。这里我们只讨论与拥塞相关的响应算法。在首次发生超时重传时，Eifel 算法开始执行。若认为出现伪
重传情况，会撤销对 ssthresh 值的修改。在所有情况下，若因 RTO 而需改变 ssthresh 值，在修改前需要记录一个特殊变量：pipe_prev = min（在外数据值，ssthresh）。然后需要运行一
个检测算法（即之前提到的检测方法中的某个）来判断 RTO 是否真实。假如出现伪重传，则当到达一个 ACK 时，执行以下步骤：

\begin{enumerate}
    \item 若接收的是包含ECN-Echo 标志位的好的ACK，停止操作（参见16.11节）。
    \item cwnd =在外数据值+ min （bytes_acked, IW）（假设cwnd 以字节为单位）。
    \item ssthresh = pipe_prevo
\end{enumerate}

在改变 ssthresh 之前需要设置pipe_prev 变量。pipe_prev 用于保存ssthresh 的记录值，以便在步骤3中重设 ssthresh。步骤1针对带 ECN 标志位的ACK 的情况（在16.11 节中将详
细讨论 ECN）。这种情况下撤销 ssthresh 修改会引入不安全因素，所以算法终止。步骤2和步骤3是算法的主要部分（针对cwnd）。步骤2将cwnd设置 一定值，允许不超过IW 的新
数据进入传输通道。因为即使在未知链路拥塞与否的状况下，发送IW 的新数据也被认为是安全的。步骤3在真正的 RTO 发生前重置 ssthresh，至此撤销操作完成。

\section{扩展举例}
下面我们通过一个例子来演示一下前面章节提到的操作算法。利用sock 程序，在一条DSL 线路上传输 2.5MB 数据。发送方和接收方分别为 Linux （2.6）和 FreeBSD（5.4）。链
路在发送方向上限速约为300Kb/s。FreeBSD 接收端处于高带宽连接。发送端至接收端的最小 RTT为15.9ms，需经17个跳步。大部分处理操作均使用基础算法（如慢启动和拥塞避
免），以避免不同操作系统的实现细节差异（后面我们会提到相关问题）。下面开始实验，首先在接收端运行如下操作命令：

\begin{verbatim}
    FreeBSD8
sock -1 -r 32768 -R 233016 -8 6666
\end{verbatim}

该命令为sock 程序设置了一个较大的套接字接收级存（228KB），并执行大数据量的读操作（32KB）。对于传输链路来说，接收缓存已足够大。接着设置发送端为发送模式，命令
如下：

\begin{verbatim}
    Linux? sock -n20 -i -w 131072 -S 262144 128.32.37.219 6666
\end{verbatim}

该命令选择了一个较大的发送级存并发送了20×131 072 字节（2.5MB）数据。利用发送端的 topdump 可以记录数据包的传输軌迹，命令如下：
\begin{verbatim}
    Linux# tcpdump -s 128 -w sack-to-free-12.td port 6666
\end{verbatim}

该命令确保每个数据包至少记录 128字节，对获取 TCP 和IP 头部信息已足够。得到相关记录后，可以采用工具 tcptrace ［TCPTRACE］来收集连接相关的统计信息，命令如下：
\begin{verbatim}
    Linux? toptrace -Wl sack-to-free-12.td
\end{verbatim}
该命令需要提供拥塞窗口的相关信息，其输出格式较长（详细），输出如下：

从上述输出中可以得到很多连接方面的信息。我们首先关注输出的左半部分（a->b）。可以看到在a~b方向上共传输了1903个包，其中1902个为ACK。这和预计是相符的，因为通
常第一个传输的包是SYN—唯一一个没有 ACK 标志位的包。纯ACK 包是指不包含传输数据的包。发送端一共发送了两个纯 ACK 包，一个是在连接初始阶段响应接收端发送的 SYN+
ACK 包，另一个是在连接结束时发送的。第二栏（b->a方向）显示，接收端共发送了1272个包，全部是ACK。其中，1270个是纯ACK 包，并有79个是SACK 包（即包含SACK选项的
ACK）。两个“不纯”的ACK 分别是连接之初的SYN +ACK 以及结束时的FIN +ACK。

从下面的5个值可以看出部分数据经过了重传。可以看到，单次传输的数据为2621440字节（即没有重传），但总的传输量达到了2659240字节，说明有2659240-
2621 440 -37 800字节数据经历了多于一次的传输。接下来的两个字段验证了这一点，这些数据被分成27个数据包进行重传，平均每个重传数据段大小为1399字节。由于在 100.476s
时间内完成了2659240 字节的传输，平均吞吐量为 26466B/s（约212kb/s）。平均优质吞吐量（goodput，即单位时间内无重传的数据量）为2621 440/100.476=26 090B/S，约209kb/S。
可以看出，传输性能受到了严重干扰。我们可以利用 Wireshark 查看TCP 操作并分析干扰产生的原因。

为得到记录结果的图像，可以使用 Wireshark 的统计菜单中的“统计 ITCP 流图| 时间序列图”（Statistics | TCP Stream Graph | Time-Sequence Graph） 功能（tcptrace），如图16-4所示
（为方便讨论已用箭头标记）。

图16-4的y轴表示 TCP 序列号，每小格代表100000个序列号。x 轴是时间，以秒为单位。黑体实线由许多小的1字形线段组成，每段代表TCP 序列号范围。1形线段的最高
点表示用户数据负载大小，以字节为单位。线段的斜率为数据到达速率。斜率减小表示出现重传。在给定时间范围内的线段斜率，代表了该时间段内的平均吞吐量。从图中可以看
到，在100s时刻，发送的最大序列号为260000，表示粗略地对平均优质吞吐量的估计值为26 000B/s，这与前面对 tcptrace 输出结果的分析相一致。

图中上方曲线为接收端在对应时刻可接收数据的最大序列号（最大通知窗口）。可以看到，在起始时刻，其值约为 250000， teptrace 输出中的b->a 栏中的精确数据显示为233016。
下方曲线代表发送端在对应时刻接收到的最大 ACK 号。之前已经提到过，当 TCP 进行操作时，会增大拥塞窗口，以获取新的带宽。这和接收端的通告窗口并不冲突。这一点可以从
图中看出，随着时间推移，实线部分逐渐从下方曲线向上方曲线靠近。若始终达不到上方曲线，影响网络吞吐量的主要因素可能为发送端或者网络传输资源的限制。若黑线部分始终紧
贴下方曲线，则影响因素主要在于接收窗口限制。

Linux 2.6.10 TCP 发送端传输 2.5MB文件的 Wireshark 记录，DSL 线路速率约为300Kb/s。黑体实线代表发送序列号。上方曲线为接收端通知窗口的最高序列号（窗口右边界），下方曲线
表示发送方接收到的最大 ACK 号。图中标记的11个事件为拥塞窗口的变化情况

\subsection{慢启动行为}
在分析之前，首先观察我们之前介绍过的慢启动算法的相关操作。在Wireshark 中选择记录结果的第一个包，利用菜单中的“统计|流图”（Stafistes| Flow Graph）功能，捕绘出在
连接初始阶段包交换的过程（参见图16-5）。

从图中可以看到初始阶段的SYN 和SYN+ACK 的交换过程。0.032s 时刻的ACK 是一次窗口更新（参见第15 章）。前两次数据包传输出现在0.126s和0.127s时刻。在0.210s时
刻返回的ACK 不是仅对一个数据包的确认。它的序列号为2801，由于 TCP ACK 的累积确认性质，它是对前两个发送的数据包的响应。这是延时ACK 的一个例子，延时ACK 通常是
每两个数据包生成一个 ACK（或如［RFC5681］ 中建议的更频繁）。对接收端（FreeBSD 5.4）来说更为特殊，它需要在每个ACK 确认一个包和两个包之间切换。这表明平均来说，每三
个数据包会返回两个 ACK（假设没有出现传输错误和重传）。在第15 章中我们已经讨论过延时 ACK 和窗口更新问题了。

-个 ACK 完成对两个数据包的确认，使得滑动窗口可以向前滑动两个包，因此可以继续发送两个新的数据包。由于连接处于初始慢启动阶段，发送端每接收一个好的 ACK，
拥塞窗口相应加1（Linux TCP 管理的拥塞窗口以包为单位）。在上述情况下，cwnd 从2增至3。因此可以继续传输三个数据包，分别在0.2158、0.216s 和0.217s时刻发送。

0.264s到达的ACK 是对单个包的确认，表明接收方期望下次接收序列号为4201 的数据包。然而，4201 号以及之后的5601 号数据包已被发送，但仍未到达。因此，0.264S时刻的
ACK使得cwnd 由3变力4，但由于两个包仍处于传送状态，只能允许继续发送两个数据包（该ACK 使得滑动窗口前行，另外，接收到这个好的ACK 允许
cwnd 加1）。这两个包的发送时间为 0.268s和 0.268s（在同一个 1/1000秒内）。

以上是发送端执行慢启动情况下接收端延时返回 ACK 的典型例子。这个过程持续（每接收一个 ACK 发送两三个新数据包）直到5.6S。下面我们进一步讨论此时发生的情况。

\subsection{发送暂停和本地拥塞（事件1）}

如图16-4所示，在5.512s 时刻发送一个数据段后，直到6.162s 时刻才开始再次发送，这中间出现了一个暂停。利用Wireshark 的图像放大功能可以得到图16-6。

可以看到，在发送暂停阶段没有新数据的传输，也没有重传，但暂停结束后却出现了传输速率的下降，这是为什么呢？我们再次通过传输流记录功能一探究竟（参见图16-7）。

暂停前最后一次传输的数据段开启了 PSH标志，表明发送缓存已经清空，所以在S.SS9s时刻TCP发送端已经终止发送。导致发送终止的原因可能有多种，如发送方系统忙于
处理其他任务，无暇顾及数据传输。

我们可以看到这次暂停并不意味着重传恢复阶段的开始，但暂停结束后线段的斜率有所下降，表明发送速率在减小。下面将仔细观察并探讨这种行为产生的原因。

暂停前最后发送数据的序列号为343001+1400-1=344400，该序列号之前没有发送过，所以不是重传数据。在5.486s时刻（已标记出）发送完数据段后，网络中己发出但未收
到 ACK 的数据量达到最大值：341 601 +1400- 205 801 = 137 200字节（98个包），即 cwnd值为98个包。5.556s时刻到达的 ACK 表明又有两个包被成功接收。暂停前最后又发送了一
个数据包，序列号为344400，这样一共有97个包还来成功接收。

在慢启动阶段后，连接暂停持续了约512ms，接着在5.512s时刻恢复发送

在5.5S9s 时刻发送方暂停发送。6.209s时刻重新开始发送，由于本地拥塞，可发送包个数被限制为8个。有些TCP 版本就是利用限制发送速率的方法来避免发送端队列拥塞

在发送暂停阶段，共有11个ACK 到达（之前提过，每个 ACK确认一个或两个数据段）。最后一个 ACK 表明序列号为233800的数据段已成功传输，同时仍有110600字节
（79个包）的数据没有收到确认。此时，发送方开始继续发送，它可以发送的数据包个数为98-79=19个，但从图中看到，它只发送了8个。至6.128s，它发送的数据段的最终序列
号为354201+1400-1=355600。

从传输流记录图中并不能很清楚地看到 TCP 当时的状况。我们预料应该会发送19个包，但结果只发送了8个。原因可能在于，下层产生的大量数据包堵塞了本地（下层）队列，
使得后续包无法传送。为明确是否由下层原因导致上述问题，由于数据包经过 PppO网络接口传输，所以在 Linux 中使用如下命令：
\begin{verbatim}
    Linuxs te -s -d adise show dev pppO gdisc pfifo_fast 0: bands
3 prionap
1222120011111111
Sent 122569547 bytes 348574 pkts (dropped 2,
overlimits
0 requeues 0)
\end{verbatim}

上述命令中的tc 是Linux 中用于管理包调度和流量控制子系统的指令［LARTC］。-s 和-d选项提供具体的记录细节。指令 qdise show dev pppO 为显示设备pppO的排队规则，即管理
和调度包发送的方法。注意到这里出现了两个丢包，这不是在网络传输过程中的丢包，而是出现在发送端 TCP 下层的丢包。由于丢包发生于 TCP 层以下，但又是在包的操作处理层以
上，所以传输流记录中并不记录这些包，这也是我们只看到8个包传输的原因。在发送端系统产生的丢包有时称为本地拥塞，产生原因在于，TCP产生数据包的速度大于下层队列的发
送速度。

\begin{tcolorbox}
    Linux流量控制子系统以及一些路由器和操作系统支持的优先级策略或 QoS特性（如 Microsoft 的qWave API［WQOS］），可能使用不同的排队规则，按照数据
    包的特性（如IP DSCP 值或 TCP端口号）会有不同的调度方法。对某些数据包（如多媒体数据包，TCP纯ACK 包等）采用优先级策略，可以提升交互式应用的用户
    体验。一般来说，互联网并不支持优先级策略，但许多局域网和有些企业 IP 网络中会采用这种策略。
\end{tcolorbox}

本地拥塞是 Linux TCP 实行拥塞窗口缩减（Congestion Window Reducing, CWR）策略［SK02］的原因之一。首先，将 ssthresh 值设置为cwnd/2，并将 cwnd设力min （cwnd.，在
外数据值+1）。在CWR 阶段，发送方每接收两个 ACK 就将cwnd 减1，直到cwnd达到新的ssthresh 值或者由于其他原因结束CWR（如出现丢包）。这本质上和前面提到的速率减半
（rate-halving） 算法一致。若 TCP 发送端接收到 TCP 头部中的 ECN-Echo 也会进入CWR状态（参见16.1.1节）。

了解了这些之后，我们就可以理解前面情况的产生原因了。当 TCP 结束暂停后，它只能继续发送8个包。由于本地拥塞，无法传输额外的包，TCP进入CWR状态。ssthresh 立
即减为98/2 =49个包，cwnd 也变为79+8=87个包。每接收两个 ACK，cwnd 就会减1，这样就导致发送速率减慢，直到8.364s时刻cwnd 值变为66个包。

发送速率的减小也可以从图16-6中观察出来，在5.5s 时刻前，线段的斜率显示数据传输速率约为 500Kb/S。这个值大于传输方向上的最大速率，必然会使得链路中的一个或多
个队列出现拥堵，导致 RTT增大。我们可通过“统计ITCP 流图IRTT 图”（Statistics | TCPStream Graph | Round Trip Time Graph） 进行观察（参见图16-8）。
